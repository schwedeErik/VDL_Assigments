\input{../includes/preamble}

\title{Exercise 3 - Recurrent Networks and NLP}
\date{December 2023}

\begin{document}

\maketitle
\section*{3.1. Backpropagation through Time}
\subsection*{i)}
Show that: 
\[\frac{\partial L}{\partial W} = \sum_{t=1}^{T} 
\sum_{k=1}^{t} 
\frac{\partial L_t}{\partial h_t} 
\frac{\partial h_t}{\partial h_k} 
\frac{\partial h_k}{\partial W} \]\\
The loss functions is defined as:
\[L=\sum_{t=1}^{T} L_t\]
\[\frac{\partial L}{\partial W} = \sum_{t=1}^{T}\frac{\partial L_t}{\partial W}\]
Lets expand the sum:
\[ \frac{\partial L_t}{\partial W} =
  \frac{\partial L_t}{\partial h_t} \cdot (\frac{\partial h_t}{\partial W} +
  \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial W} + . . . +
  \frac{\partial h_t}{\partial h_1} \cdot \frac{\partial h_1}{\partial W})\]
%\[ \frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial h_3} \sum_{k=1}^{3} \frac{\partial h_3}{\partial h_k} \cdot \frac{\partial h_k}{\partial W}\]
\[ \frac{\partial L_t}{\partial W} = \frac{\partial L_t}{\partial h_t} \sum_{k=1}^{t} \frac{\partial h_t}{\partial h_k} \cdot \frac{\partial h_k}{\partial W}\]
\[\frac{\partial L}{\partial W} = \sum_{t=1}^{T}\frac{\partial L_t}{\partial h_t} \sum_{k=1}^{t} \frac{\partial h_t}{\partial h_k} \cdot \frac{\partial h_k}{\partial W}\]

\[\frac{\partial L}{\partial W} = \sum_{t=1}^{T} 
\sum_{k=1}^{t} 
\frac{\partial L_t}{\partial h_t} 
\frac{\partial h_t}{\partial h_k} 
\frac{\partial h_k}{\partial W} \]
\subsection*{ii)}
Given:
\[f(h) = \sigma(W \cdot h)\]
Show that:
\[\frac{\partial f}{\partial h} = diag(\sigma^{\prime}(Wh))W\]
\subsection*{iii)}


\subsection*{iv)}


\section*{3.2. Gated recurrent units}
\subsection*{i)}

Given:

\begin{alignat}{2}
u_t = \sigma (w.h_{t-1} + w.x_t)
\\s_t = w. (h_{t-1} + x_t)
\\h_t = u_t. h_{t-1}  + (1-u_t) . s_t
\end{alignat}

Calculating derivative of s from equation 2:
\begin{alignat}{3}
\\s_t = w. (h_{t-1} + x_t)
\end{alignat}

\begin{alignat}{3}
\frac {\partial s_t}{\partial h_{t-1}} &= w.\frac{\partial h_{t-1}}{\partial h_{t-1}} + 0 
\\ &= w
\end{alignat}

Let's differentiate equation (3) with respect to ht−1:

\begin{alignat}{3}
\frac {\partial h_t}{\partial h_{t-1}} &= u_t +   \sigma'(w.h_{t-1}+w.x_t)h_{t-1} + (1-u_t)w - s_t.w.\sigma'(w.h_{t-1}+w.x_t)
\\ &= w(\sigma'(w.h_{t-1} + w.x_t)h_{t-1} + (1-u_t) - s_t.\sigma'(w.h_{t-1} + w.x_t) + u_t
\end{alignat}

Since we know about the derivative of the sigmoid function - 

\begin{alignat}{3}
\sigma' = \sigma(1-\sigma)
\end{alignat}

On substituting it back to our previous equation, we have:

\begin{alignat}{3}
\frac {\partial h_t}{\partial h_{t-1}} &= w.(u_t(1-u_t)h_{t-1} + (1-u_t) - s_t.u_t(1-u_t) +u_t
\\ &= w(1-u_t)(h_{t-1}.u_t-s_t.u_t +1) + u_t
\end{alignat}


On comparing it with the required form A\textsubscript{t} . w + B\textsubscript{t} , we get:

\begin{alignat}{3}
A_t &= (1-u_t)(h_{t-1}u_t - s_t.u_t+1)
\\B_t &= u_t
\end{alignat}

\subsection*{ii)}

The long-term derivative can be written as:

\begin{alignat}{3}
\frac {\partial h_t}{\partial h_0} &= \frac {\partial h_t}{\partial h_{t-1}} . \frac {\partial h_{t-1}}{\partial h_{t-2}}. \frac {\partial h_{t-2}}{\partial h_{t-3}}...... \frac {\partial h_1}{\partial h_0} 
\\or
\\
\frac {\partial h_t}{\partial h_0} &= \prod_{i=0}^{t} \frac{\partial h_i}{\partial h_{i-1}}
\end{alignat}

From part (i) we have found that, for every term:

\begin{alignat}{3}
\frac {\partial h_t}{\partial h_{t-1}} &= w(1-u_t)(h_{t-1}.u_t-s_t.u_t +1) + u_t
\end{alignat}\\
u\textsubscript{t} is the output of a sigmoid function that has a value in the range of (0,1). If the value of u\textsubscript{t} is close to zero, the weight term won’t allow it to disappear. This helps to avoid the vanishing gradient problem, as no exponential decay term can cause the gradients to vanish over long sequences.\\
When u\textsubscript{t} is 1,the B\textsubscript{t} function (u\textsubscript{t}) avoids the vanishing gradient, it implies that the new hidden state retains the value of the previous hidden state and when u\textsubscript{t} is 0, it ignores the information from the previous step.



\end{document}
