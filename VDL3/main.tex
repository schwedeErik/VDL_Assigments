\input{../includes/preamble}

\title{Exercise 3 - Recurrent Networks and NLP}
\date{December 2023}

\begin{document}

\maketitle
\section*{3.1. Backpropagation through Time}
\subsection*{i)}
Show that: 
\[\frac{\partial L}{\partial W} = \sum_{t=1}^{T} 
\sum_{k=1}^{t} 
\frac{\partial L_t}{\partial h_t} 
\frac{\partial h_t}{\partial h_k} 
\frac{\partial h_k}{\partial W} \]\\
The loss functions is defined as:
\[L=\sum_{t=1}^{T} L_t\]
\[\frac{\partial L}{\partial W} = \sum_{t=1}^{T}\frac{\partial L_t}{\partial W}\]
Lets expand the sum:
\[ \frac{\partial L_t}{\partial W} =
  \frac{\partial L_t}{\partial h_t} \cdot (\frac{\partial h_t}{\partial W} +
  \frac{\partial h_t}{\partial h_{t-1}} \cdot \frac{\partial h_{t-1}}{\partial W} + . . . +
  \frac{\partial h_t}{\partial h_1} \cdot \frac{\partial h_1}{\partial W})\]
%\[ \frac{\partial L_3}{\partial W} = \frac{\partial L_3}{\partial h_3} \sum_{k=1}^{3} \frac{\partial h_3}{\partial h_k} \cdot \frac{\partial h_k}{\partial W}\]
\[ \frac{\partial L_t}{\partial W} = \frac{\partial L_t}{\partial h_t} \sum_{k=1}^{t} \frac{\partial h_t}{\partial h_k} \cdot \frac{\partial h_k}{\partial W}\]
\[\frac{\partial L}{\partial W} = \sum_{t=1}^{T}\frac{\partial L_t}{\partial h_t} \sum_{k=1}^{t} \frac{\partial h_t}{\partial h_k} \cdot \frac{\partial h_k}{\partial W}\]

\[\frac{\partial L}{\partial W} = \sum_{t=1}^{T} 
\sum_{k=1}^{t} 
\frac{\partial L_t}{\partial h_t} 
\frac{\partial h_t}{\partial h_k} 
\frac{\partial h_k}{\partial W} \]
\subsection*{ii)}
Given:
\[f(h) = \sigma(W \cdot h)\]
Show that:
\[\frac{\partial f}{\partial h} = diag(\sigma^{\prime}(Wh))W\]
\subsection*{iii)}


\subsection*{iv)}


\section*{3.2. Gated recurrent units}
\subsection*{i)}

Given:
\begin{alignat}{2}
    u_t &= \sigma (w.h_{t-1} + w.x_t)
    \\s_t &= w. (h_{t-1} + x_t)
    \\h_t &= u_t. h_{t-1}  + (1-u_t) . s_t
\end{alignat}

First, let's differentiate equation \((3\)) with respect to h\textsubscript{t-1}:

\begin{alignat}{2}
\frac {\partial h_t}{\partial h_{t-1}} &= \frac {u_t * h_{t-1} + (1-u_t) * s_t}{\partial h_{t-1}}
\end{alignat}

On applying the chain rule : 

\begin{alignat}{2}
\frac {\partial h_t}{\partial h_{t-1}} &= u_t . \frac {\partial h_{t-1}}{\partial h_{t-1}} + (1-u_t).\frac {\partial s_t}{\partial h_{t-1}}
\end{alignat}

We have the value of s from equation 2:

\begin{alignat}{2}
\\s_t &= w. (h_{t-1} + x_t)
\\ \frac {\partial s_t}{\partial h_{t-1}} &= w.\frac{\partial h_{t-1}}{\partial h_{t-1}} + 0 
\\ &= w
\end{alignat}

On substituting it back to our previous equation, we have:

\begin{alignat}{2}
\frac {\partial h_t}{\partial h_{t-1}} &= u_t + (1-u_t).w
\end{alignat}

On comparing it with the required form A\textsubscript{t} . w + B\textsubscript{t} , we get:

\begin{alignat}{2}
    A_t &= 1-u_t
\\ B_t &= u_t
\end{alignat}

\subsection*{ii)}
The long-term derivative can be written as:
\begin{alignat}{2}
\frac {\partial h_t}{\partial h_0} &= \frac {\partial h_t}{\partial h_{t-1}} . \frac {\partial h_{t-1}}{\partial h_{t-2}}. \frac {\partial h_{t-2}}{\partial h_{t-3}}...... \frac {\partial h_1}{\partial h_0} 
\end{alignat}
From part (i) we have found that, for every term:

\begin{alignat}{2}
\frac {\partial h_t}{\partial h_{t-1}} = (1-u_t) .w + u_t
\end{alignat}
u\textsubscript{t} is the output of a sigmoid function that has a value in the range of (0,1). Hence, the term remains non-zero. This helps to avoid the vanishing gradient problem, as no exponential decay term can cause the gradients to vanish over long sequences.
When u\textsubscript{t} is 1, it implies that the new hidden state retains the value of the previous hidden state and when u\textsubscript{t} is 0, it ignores the information from the previous step. 


\end{document}
